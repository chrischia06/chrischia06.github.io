



**Explaining Recurrent Neural Network Predictions in Sentiment Analysis, L. Arras, G. Montavon, K.-R. MÃ¼ller and W. Samek WASSA@EMNLP'2017 [arXiv:1706.07206]**
+ https://www.aclweb.org/anthology/W17-5221/
+ https://github.com/ArrasL/LRP_for_LSTM
+ A way to explain the predictions of a LSTM



**Allen-Zhu, Z., Li, Y., Backward Feature Correction: How Deep Learning Performs Deep Learning (2020)** 

+ https://arxiv.org/abs/2001.04413

+ Article: **Three mysteries in deep learning: Ensemble, knowledge distillation, and self-distillation**
+ https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/
+ Functional forms from Neural Networks with the same architecture, but even different seeds are very different!
+ Knowledge distillation: train another model to learn the output of the ensemble




**Yang, G., Hu. E., Feature Learning in Infinite-Width Neural Networks (2020)**

https://arxiv.org/abs/2011.14522

**Domingos, P., Every Model Learned by Gradient Descent Is Approximatelya Kernel Machine**

https://arxiv.org/pdf/2012.00152.pdf




**Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning**
+ https://www.researchgate.net/publication/328107188_Implicit_Self-Regularization_in_Deep_Neural_Networks_Evidence_from_Random_Matrix_Theory_and_Implications_for_Learning

**Traditional and Heavy Tailed Self Regularization in Neural Network ModelsDownload PDF (2018)**
+ https://openreview.net/forum?id=SJeFNoRcFQ
+  https://pypi.org/project/weightwatcher/


**TabNet: Attentive Interpretable Tabular Learning**
+ https://arxiv.org/abs/1908.07442
+https://github.com/google-research/google-research/tree/master/tabnet

+https://towardsdatascience.com/the-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33?gi=ebfeda323451
+ FastAI: https://www.fast.ai/2019/01/24/course-v3/



**Can stable and accurate neural networks be computed? -- On the barriers of deep learning and Smale's 18th problem**
+ https://arxiv.org/abs/2101.08286

**Deep Learning and the Information Bottleneck Principle**
+ https://arxiv.org/abs/1503.02406

**On the Information Bottleneck Theory of Deep Learning**
+ https://openreview.net/forum?id=ry_WPG-A-

**Opening the Black Box of Deep Neural Networks via Information**
+ https://arxiv.org/abs/1703.00810

**The Information Bottleneck Method**
+ https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf

**Efficient Approximation of Deep ReLU Networks forFunctions on Low Dimensional Manifolds**
+ https://openreview.net/pdf/a7214aa6dce0e71061cc0251ce2bc1f3d5c8f3af.pdf

**Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data**
+ https://arxiv.org/abs/1909.06312

**Denise: Deep Learning based Robust PCA forPositive Semidefinite Matrices**
https://arxiv.org/pdf/2004.13612.pdf

**How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks**

+ https://arxiv.org/abs/2009.11848

**Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning (2018)**

+ https://arxiv.org/pdf/1810.01075.pdf

**Traditional and Heavy-Tailed Self Regularization in Neural Network Models (2020)**

+ https://arxiv.org/pdf/1901.08276.pdf

**Predicting trends in the quality of state-of-the-art neural networks without access to training or testing dat (2020)**

+ https://arxiv.org/pdf/2002.06716.pdf




## Historical
+ Hornik (1990) Universal Approximation Theorem


